{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravishankar75/group44_cv_assignment/blob/main/CV_assignment_1_group44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bda93ba",
      "metadata": {
        "id": "9bda93ba"
      },
      "source": [
        "## Group 44\n",
        "\n",
        "## Group Member Names:\n",
        "1. SAKTHI R (2023aa05940)\n",
        "2. ROBERTSEKAR R (2023aa05823)\n",
        "3. RAVISHANKAR R (2023aa05171)\n",
        "4. KRISHNAKUMAR C (2023aa05273)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Link: https://drive.google.com/file/d/18ivVD85YKQqPH0Qhe2Ou10hjuPl-vWxA/view?usp=sharing\n",
        "\n",
        "\n",
        "Choose any 1 dataset of your choice to perform the assignment."
      ],
      "metadata": {
        "id": "nxkuZrLQN7lH"
      },
      "id": "nxkuZrLQN7lH"
    },
    {
      "cell_type": "markdown",
      "id": "bec56339",
      "metadata": {
        "id": "bec56339"
      },
      "source": [
        "# 1. Import the required libraries -- Score: 0.5 Marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "46631608",
      "metadata": {
        "id": "46631608"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2\n",
        "from skimage.feature import local_binary_pattern #opencv2 does not expose LBP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc8e0cb",
      "metadata": {
        "id": "3cc8e0cb"
      },
      "source": [
        "# 2. Data Acquisition & Preparation -- Score: 1.5 Marks\n",
        "\n",
        "For the problem identified by you, students have to find the data source themselves from any data source.\n",
        "\n",
        "## 2.1 Data Acquisition -- Score: 0.5 Mark\n",
        "\n",
        "Code for converting the above downloaded data into a form suitable for DL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4b51d895",
      "metadata": {
        "id": "4b51d895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb500c12-e11c-4adb-a764-6a47df699100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompression complete., Files extracted to /content/data\n",
            "Category: Forest, Number of Images: 2745\n",
            "Category: Streets, Number of Images: 501\n",
            "Category: Building, Number of Images: 501\n",
            "Category: Sea, Number of Images: 501\n",
            "Category: Mountains, Number of Images: 501\n",
            "Category: Glacier, Number of Images: 501\n"
          ]
        }
      ],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "\n",
        "# Shared dataset link not working with gdown library due to permissions\n",
        "# The images have been downloaded and uploaded to Google drive(My).\n",
        "\n",
        "# The below steps necessary as on reconnect, google collab deletes the content folder\n",
        "\n",
        "colab_folder = \"/content\"  # Default working directory in Google Colab\n",
        "zip_file_name = \"/content/drive/MyDrive/scene_classification.zip\"\n",
        "\n",
        "\n",
        "# Decompress the zip file, using with instead of try/finally\n",
        "# check if data folder exists, if it exists then the dataset has been extracted\n",
        "\n",
        "if not os.path.exists(os.path.join(colab_folder, \"data\")):\n",
        "\n",
        "  print(\"Mounting Google Drive...\")\n",
        "  drive.mount('/content/drive') # Approve access for google desktop app\n",
        "\n",
        "  print(\"Decompressing the zip file...\")\n",
        "  with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "      extract_path = os.path.join(colab_folder, \"data\")\n",
        "      zip_ref.extractall(extract_path)\n",
        "\n",
        "print( f\"Decompression complete., Files extracted to {extract_path}\")\n",
        "\n",
        "# find the number of images for each category\n",
        "\n",
        "for category in os.listdir(os.path.join(colab_folder, \"data\", \"subset\")):\n",
        "  category_path = os.path.join(colab_folder, \"data\", \"subset\", category)\n",
        "  if not os.path.isdir(category_path):\n",
        "    continue\n",
        "  num_images = len(os.listdir(category_path))\n",
        "  print(f\"Category: {category}, Number of Images: {num_images}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to delete after completion\n",
        "\n",
        "# Define constants\n",
        "NUM_IMAGES_PER_FOLDER = 5\n",
        "DATASET = []\n",
        "\n",
        "# Features to extract from images - Color, Shape and Texture\n",
        "\n",
        "# Function to resize Image, convert all images to standard size\n",
        "def resize_image(image):\n",
        "    IMAGE_SIZE = (128, 128)  # Resize to 128x128\n",
        "    resized_image = cv2.resize(image, IMAGE_SIZE)\n",
        "    return resized_image\n",
        "\n",
        "# Function to extract color details\n",
        "# Forests are mostly green, Sea blue, Glaciers White etc\n",
        "def extract_color_details(image):\n",
        "   # OpenCv2 loads in BGR format, calculate histogram of intensity\n",
        "   # Mask = None, All 3 channels, 8 Bins, Range of Intensity (0 to L-1) L=256\n",
        "   hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
        "   hist = cv2.normalize(hist, hist).flatten()\n",
        "   return hist\n",
        "\n",
        "# Function to extract texture\n",
        "# Texture refer to spatial arrangement of intensity or color patterns in an image\n",
        "# Helps identify if there are irregular patterns (Mountain), uniform patterns (Sea, glacier), repeating patterns (Building, Street) etc\n",
        "# Methods - Local Binary Factor, Gibor Filter, GLCM not incl- computational cost\n",
        "\n",
        "def extract_texture_LBP(gray_image):\n",
        "   # Input image has to be  grayscale\n",
        "   # Compute the LBP of the image, using skimage.feature.localBinaryPattern\n",
        "   # P=8, R=3.0\n",
        "   lbp = local_binary_pattern(gray_image, 8, 3.0, method=\"uniform\")\n",
        "\n",
        "   return lbp\n",
        "\n",
        "def extract_texture_gabor(gray_image, psi):\n",
        "  # Input image has to be  grayscale\n",
        "  # Apply Gabor kernel, from the resultant image pixels, calculate mean & stddev\n",
        "\n",
        "   gabor_kernel = cv2.getGaborKernel(ksize=(5, 5), sigma=5.0, theta=0.0, lambd=10.0, gamma=0.5, psi=psi)\n",
        "\n",
        "   filtered_image = cv2.filter2D(gray_image, cv2.CV_8UC3, gabor_kernel)\n",
        "\n",
        "   mean = np.mean(filtered_image)\n",
        "   std_dev = np.std(filtered_image)\n",
        "   return (mean, std_dev)\n",
        "\n",
        "# Function to extract shape features\n",
        "# Determine edges, Buildings/Street should have more edges than Sea/Glacier\n",
        "# Use measure = no of edge pixels / total pixels in image\n",
        "\n",
        "def extract_edge_features(gray_image):\n",
        "\n",
        "  # detect edges using Canny algo\n",
        "  edge_image = cv2.Canny(gray_image, threshold1=50, threshold2=150)\n",
        "  no_of_edge_pixel = np.Sum(edge_image > 0 ) # Any non black pixel\n",
        "\n",
        "  return no_of_edge_pixel/gray_image.size\n",
        "\n",
        "# Path to the base folder containing category subfolders\n",
        "base_folder = os.path.join(colab_folder, \"data\", \"subset\")\n",
        "\n",
        "# Iterate through category folders\n",
        "for category in os.listdir(base_folder):\n",
        "    category_path = os.path.join(base_folder, category)\n",
        "    if not os.path.isdir(category_path):\n",
        "        continue\n",
        "\n",
        "    # Process up to NUM_IMAGES_PER_FOLDER images in this category\n",
        "    image_count = 0\n",
        "    for image_file in os.listdir(category_path):\n",
        "        if image_count >= NUM_IMAGES_PER_FOLDER:\n",
        "            break\n",
        "\n",
        "        # Full path to image file\n",
        "        image_path = os.path.join(category_path, image_file)\n",
        "        if not os.path.isfile(image_path):\n",
        "            continue\n",
        "\n",
        "        # Process the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        if image is None:\n",
        "            continue\n",
        "        # Convert image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        dominant_colors = extract_color_details(image)\n",
        "        texture_lbp = extract_texture_LBP(gray_image)\n",
        "        texture_gabor_0 = extract_texture_gabor(gray_image, 0)\n",
        "        texture_gabor_45 = extract_texture_gabor(gray_image, 45)\n",
        "        texture_gabor_90 = extract_texture_gabor(gray_image, 90)\n",
        "\n",
        "        edge_density = extract_edge_features(gray_image)\n",
        "\n",
        "\n",
        "        # Append data to dataset\n",
        "        DATASET.append({\n",
        "            \"filename\": image_path,\n",
        "            \"label\": category,\n",
        "            \"dominant_colors\": dominant_colors.tolist(),\n",
        "            \"texture_lbp\": texture_lbp.tolist(),\n",
        "            \"texture_gabor_0\": texture_gabor_0,\n",
        "            \"texture_gabor_45\": texture_gabor_45,\n",
        "            \"texture_gabor_90\": texture_gabor_90,\n",
        "            \"edge_density\": edge_density\n",
        "        })\n",
        "        image_count += 1\n",
        "\n",
        "# Convert dataset to a DataFrame\n",
        "df = pd.DataFrame(DATASET)\n",
        "\n",
        "# Save dataset to CSV\n",
        "df.to_csv(\"image_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Dataset created and saved to 'image_dataset.csv'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIRncY-fB0jn",
        "outputId": "78a02fdf-3380-48f5-feeb-f484ae659ce4"
      },
      "id": "fIRncY-fB0jn",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and saved to 'image_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812edb18",
      "metadata": {
        "id": "812edb18"
      },
      "source": [
        "## 2.2 Write your observations from the above.\n",
        "\n",
        "1. Size of the dataset\n",
        "2. Plot the distribution of the categories of the target / label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102e0e36",
      "metadata": {
        "id": "102e0e36"
      },
      "source": [
        "## 2.2 Data Preparation -- Score: 1.0 Marks\n",
        "\n",
        "Perform the data preprocessing that is required for the data that you have downloaded.\n",
        "\n",
        "\n",
        "This stage depends on the dataset that is used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd14601",
      "metadata": {
        "id": "4cd14601"
      },
      "source": [
        "## 3.1 Split the data into training set and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a74cd9c",
      "metadata": {
        "id": "1a74cd9c"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3cec4fc",
      "metadata": {
        "id": "e3cec4fc"
      },
      "source": [
        "## 3.2 Feature Engineering -- Score: 3.5 Marks\n",
        "\n",
        "* Extract the features from the images and concatenate them to create a single for the every images.\n",
        "\n",
        "* You can choose from the feature processing techniques taught in the class : Low-level Vision: Histogram and Histogram equalization, Gray-scale transformation, Image Smoothing, Connected components in images.\n",
        "Mid-level Vision:  Edge Detection using Gradients, Sobel, Canny; Line detection using Hough transforms; Semantic information using RANSAC;Image region descriptor using SIFT; Use case: Pedestrian detection Using HoG and SIFT descriptors and SVM\n",
        "\n",
        "* Create multiple sets of features and store it in seperate dataframes so that you can later use it for training and comparing the models.\n",
        "\n",
        "* Normalize the DataFrame\n",
        "\n",
        "* Note : If the feature size is getting too large such that it is not fitting into the RAM of Colab or your system then you can either use PCA or resize the image to smaller dimenssion for reducing the numer of features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c77697",
      "metadata": {
        "id": "71c77697"
      },
      "outputs": [],
      "source": [
        "##---------Type the answer below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae0b5d2",
      "metadata": {
        "id": "3ae0b5d2"
      },
      "source": [
        "# 4. Model Building - Score: 2.0 Marks\n",
        "\n",
        "## 4.1 Model Building - Score: 1.5 Marks\n",
        "* Use any 1 classical machine learning algorithm such as : SVM , Xgboost etc. to train the model\n",
        "* Train the model on different kinds of feature combination dataframe you created in 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868d7b27",
      "metadata": {
        "id": "868d7b27"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575f9e37",
      "metadata": {
        "id": "575f9e37"
      },
      "source": [
        "## 4.2 Validation matrix - Score: 0.5 Marks\n",
        "\n",
        "Print the model accuracy and F1 Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad56d90",
      "metadata": {
        "id": "6ad56d90"
      },
      "outputs": [],
      "source": [
        "##---------Type the answer below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdbc82a1",
      "metadata": {
        "id": "bdbc82a1"
      },
      "source": [
        "# 5. Model Inference & Evaluation - Score: 1 Mark\n",
        "\n",
        "Plot any 5 random test images and their predicted and actual true labels using the model and feature set which gave you the best accuracy/F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85e9754",
      "metadata": {
        "id": "a85e9754"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19bd0c56",
      "metadata": {
        "id": "19bd0c56"
      },
      "source": [
        "Justify your choice/inution of feature selection based on the performance of model such that why a particualr set have features might have performed well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89d2981",
      "metadata": {
        "id": "b89d2981"
      },
      "outputs": [],
      "source": [
        "##---------Type the answers below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Documentation, Study presentation and Code Quality -- Score: 1.5 Marks"
      ],
      "metadata": {
        "id": "rJBZ1ovpKgAR"
      },
      "id": "rJBZ1ovpKgAR"
    },
    {
      "cell_type": "markdown",
      "id": "RcDDQlfbZQ7E",
      "metadata": {
        "id": "RcDDQlfbZQ7E"
      },
      "source": [
        "### NOTE\n",
        "\n",
        "\n",
        "All Late Submissions will incur a <b>penalty of -2 marks </b>. So submit your assignments on time.\n",
        "\n",
        "Good Luck"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}