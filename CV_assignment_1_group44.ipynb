{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravishankar75/group44_cv_assignment/blob/main/CV_assignment_1_group44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bda93ba",
      "metadata": {
        "id": "9bda93ba"
      },
      "source": [
        "## Group 44\n",
        "\n",
        "## Group Member Names:\n",
        "1. SAKTHI R (2023aa05940)\n",
        "2. ROBERTSEKAR R (2023aa05823)\n",
        "3. RAVISHANKAR R (2023aa05171)\n",
        "4. KRISHNAKUMAR C (2023aa05273)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Link: https://drive.google.com/file/d/18ivVD85YKQqPH0Qhe2Ou10hjuPl-vWxA/view?usp=sharing\n",
        "\n",
        "\n",
        "Choose any 1 dataset of your choice to perform the assignment."
      ],
      "metadata": {
        "id": "nxkuZrLQN7lH"
      },
      "id": "nxkuZrLQN7lH"
    },
    {
      "cell_type": "markdown",
      "id": "bec56339",
      "metadata": {
        "id": "bec56339"
      },
      "source": [
        "# 1. Import the required libraries -- Score: 0.5 Marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46631608",
      "metadata": {
        "id": "46631608"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2\n",
        "from skimage.feature import local_binary_pattern #opencv2 does not expose LBP\n",
        "\n",
        "# Use Sklearn SVM or Xgboost to train the model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Use sklearn label encoder to convert to numeric value the label column\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Using Sklearn, split the dataset into training and test in 80:20 ratio\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc8e0cb",
      "metadata": {
        "id": "3cc8e0cb"
      },
      "source": [
        "# 2. Data Acquisition & Preparation -- Score: 1.5 Marks\n",
        "\n",
        "For the problem identified by you, students have to find the data source themselves from any data source.\n",
        "\n",
        "## 2.1 Data Acquisition -- Score: 0.5 Mark\n",
        "\n",
        "Code for converting the above downloaded data into a form suitable for DL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b51d895",
      "metadata": {
        "id": "4b51d895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "3cac8921-efef-4733-82bb-15052a143526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompression complete., Files extracted to /content/data\n",
            "Category: Building, Number of Images: 501\n",
            "Category: Glacier, Number of Images: 501\n",
            "Category: Sea, Number of Images: 501\n",
            "Category: Mountains, Number of Images: 501\n",
            "Category: Streets, Number of Images: 501\n",
            "Category: Forest, Number of Images: 2745\n",
            "Dataset created and saved to 'image_dataset'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2400,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0cec426116e0>\u001b[0m in \u001b[0;36m<cell line: 183>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0cec426116e0>\u001b[0m in \u001b[0;36mflatten_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mfeature_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_density'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2400,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "\n",
        "# Shared dataset link not working with gdown library due to permissions\n",
        "# The images have been downloaded and uploaded to Google drive(My).\n",
        "\n",
        "# The below steps necessary as on reconnect, google collab deletes the content folder\n",
        "\n",
        "colab_folder = \"/content\"  # Default working directory in Google Colab\n",
        "zip_file_name = \"/content/drive/MyDrive/scene_classification.zip\"\n",
        "\n",
        "\n",
        "# Decompress the zip file, using with instead of try/finally\n",
        "# check if data folder exists, if it exists then the dataset has been extracted\n",
        "\n",
        "if not os.path.exists(os.path.join(colab_folder, \"data\")):\n",
        "\n",
        "  print(\"Mounting Google Drive...\")\n",
        "  drive.mount('/content/drive') # Approve access for google desktop app\n",
        "\n",
        "  print(\"Decompressing the zip file...\")\n",
        "  with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "      extract_path = os.path.join(colab_folder, \"data\")\n",
        "      zip_ref.extractall(extract_path)\n",
        "\n",
        "print( f\"Decompression complete., Files extracted to {extract_path}\")\n",
        "\n",
        "# find the number of images for each category\n",
        "\n",
        "for category in os.listdir(os.path.join(colab_folder, \"data\", \"subset\")):\n",
        "  category_path = os.path.join(colab_folder, \"data\", \"subset\", category)\n",
        "  if not os.path.isdir(category_path):\n",
        "    continue\n",
        "  num_images = len(os.listdir(category_path))\n",
        "  print(f\"Category: {category}, Number of Images: {num_images}\")\n",
        "\n",
        "# Define constants\n",
        "NUM_IMAGES_PER_FOLDER = 500\n",
        "DATASET = []\n",
        "\n",
        "# Features to extract from images - Color, Shape and Texture\n",
        "\n",
        "# Function to resize Image, convert all images to standard size\n",
        "def resize_image(image):\n",
        "    IMAGE_SIZE = (128, 128)  # Resize to 128x128\n",
        "    resized_image = cv2.resize(image, IMAGE_SIZE)\n",
        "    return resized_image\n",
        "\n",
        "# Function to extract color details\n",
        "# Forests are mostly green, Sea blue, Glaciers White etc\n",
        "def extract_color_details(image):\n",
        "   # OpenCv2 loads in BGR format, calculate histogram of intensity\n",
        "   # Mask = None, All 3 channels, 8 Bins, Range of Intensity (0 to L-1) L=256\n",
        "   hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
        "   hist = cv2.normalize(hist, hist).flatten()\n",
        "   return hist\n",
        "\n",
        "# Function to extract texture\n",
        "# Texture refer to spatial arrangement of intensity or color patterns in an image\n",
        "# Helps identify if there are irregular patterns (Mountain), uniform patterns (Sea, glacier), repeating patterns (Building, Street) etc\n",
        "# Methods - Local Binary Factor, Gibor Filter, GLCM not incl- computational cost\n",
        "\n",
        "def extract_texture_LBP(gray_image):\n",
        "   # Input image has to be  grayscale\n",
        "   # Compute the LBP of the image, using skimage.feature.localBinaryPattern\n",
        "   # P=8, R=3.0\n",
        "   lbp = local_binary_pattern(gray_image, 8, 3.0, method=\"uniform\")\n",
        "\n",
        "   return lbp.flatten()\n",
        "\n",
        "def extract_texture_gabor(gray_image, psi):\n",
        "  # Input image has to be  grayscale\n",
        "  # Apply Gabor kernel, from the resultant image pixels, calculate mean & stddev\n",
        "\n",
        "   gabor_kernel = cv2.getGaborKernel(ksize=(5, 5), sigma=5.0, theta=0.0, lambd=10.0, gamma=0.5, psi=psi)\n",
        "\n",
        "   filtered_image = cv2.filter2D(gray_image, cv2.CV_8UC3, gabor_kernel)\n",
        "\n",
        "   mean = np.mean(filtered_image)\n",
        "   std_dev = np.std(filtered_image)\n",
        "   return mean.flatten()\n",
        "\n",
        "# Function to extract shape features\n",
        "# Determine edges, Buildings/Street should have more edges than Sea/Glacier\n",
        "# Use measure = no of edge pixels / total pixels in image\n",
        "\n",
        "def extract_edge_features(gray_image):\n",
        "\n",
        "  # detect edges using Canny algo\n",
        "  edge_image = cv2.Canny(gray_image, threshold1=50, threshold2=150)\n",
        "  no_of_edge_pixel = np.sum(edge_image > 0 ) # Any non black pixel\n",
        "\n",
        "  return no_of_edge_pixel/gray_image.size\n",
        "\n",
        "# Path to the base folder containing category subfolders\n",
        "base_folder = os.path.join(colab_folder, \"data\", \"subset\")\n",
        "\n",
        "# Iterate through category folders\n",
        "for category in os.listdir(base_folder):\n",
        "    category_path = os.path.join(base_folder, category)\n",
        "    if not os.path.isdir(category_path):\n",
        "        continue\n",
        "\n",
        "    # Process up to NUM_IMAGES_PER_FOLDER images in this category\n",
        "    image_count = 0\n",
        "    for image_file in os.listdir(category_path):\n",
        "        if image_count >= NUM_IMAGES_PER_FOLDER:\n",
        "            break\n",
        "\n",
        "        # Full path to image file\n",
        "        image_path = os.path.join(category_path, image_file)\n",
        "        if not os.path.isfile(image_path):\n",
        "            continue\n",
        "\n",
        "        # Process the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        if image is None:\n",
        "            continue\n",
        "        # Convert image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        dominant_colors = extract_color_details(image)\n",
        "        texture_lbp = extract_texture_LBP(gray_image)\n",
        "        texture_gabor_0 = extract_texture_gabor(gray_image, 0)\n",
        "        texture_gabor_45 = extract_texture_gabor(gray_image, 45)\n",
        "        texture_gabor_90 = extract_texture_gabor(gray_image, 90)\n",
        "\n",
        "        edge_density = extract_edge_features(gray_image)\n",
        "\n",
        "\n",
        "        # Append data to dataset\n",
        "        DATASET.append({\n",
        "            \"img_name\": image_path,\n",
        "            \"label\": category,\n",
        "            \"dominant_colors\": dominant_colors,\n",
        "            \"texture_lbp\": texture_lbp,\n",
        "            \"texture_gabor_0\": texture_gabor_0,\n",
        "            \"texture_gabor_45\": texture_gabor_45,\n",
        "            \"texture_gabor_90\": texture_gabor_90,\n",
        "            \"edge_density\": edge_density\n",
        "        })\n",
        "        image_count += 1\n",
        "\n",
        "# Convert dataset to a DataFrame\n",
        "df = pd.DataFrame(DATASET)\n",
        "\n",
        "# Save dataset to parquet\n",
        "df.to_parquet('image_dataset.parquet', compression='snappy')\n",
        "\n",
        "\n",
        "print(\"Dataset created and saved to 'image_dataset'\")\n",
        "\n",
        "df2 = pd.read_parquet(\"image_dataset.parquet\")\n",
        "df2.head()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'label' column in the training set\n",
        "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
        "\n",
        "\n",
        "## Extract features and labels from the DataFrames\n",
        "X_train = train_df[['dominant_colors', 'texture_lbp', 'texture_gabor_0', 'texture_gabor_45', 'texture_gabor_90', 'edge_density']]\n",
        "y_train = train_df['label_encoded']\n",
        "X_test = test_df[['dominant_colors', 'texture_lbp', 'texture_gabor_0', 'texture_gabor_45', 'texture_gabor_90', 'edge_density']]\n",
        "y_test = label_encoder.transform(test_df['label'])  # Transform test labels using the same encoder\n",
        "\n",
        "# Convert the feature columns to NumPy arrays and flatten them\n",
        "def flatten_features(df):\n",
        "    features = []\n",
        "    for _, row in df.iterrows():\n",
        "        feature_row = []\n",
        "        for col in ['dominant_colors', 'texture_lbp', 'texture_gabor_0', 'texture_gabor_45', 'texture_gabor_90']:\n",
        "            feature_row.extend(np.array(row[col]).flatten())  # Flatten each feature\n",
        "        feature_row.append(row['edge_density'])\n",
        "        features.append(feature_row)\n",
        "    return np.array(features)\n",
        "\n",
        "X_train = flatten_features(X_train)\n",
        "X_test = flatten_features(X_test)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the model on training dataset\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the data for held out test data and generate accuracy metrics\n",
        "test_predictions = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Plot the classification accuracy metrics\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(classification_report(y_test, test_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812edb18",
      "metadata": {
        "id": "812edb18"
      },
      "source": [
        "## 2.2 Write your observations from the above.\n",
        "\n",
        "1. Size of the dataset\n",
        "2. Plot the distribution of the categories of the target / label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102e0e36",
      "metadata": {
        "id": "102e0e36"
      },
      "source": [
        "## 2.2 Data Preparation -- Score: 1.0 Marks\n",
        "\n",
        "Perform the data preprocessing that is required for the data that you have downloaded.\n",
        "\n",
        "\n",
        "This stage depends on the dataset that is used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd14601",
      "metadata": {
        "id": "4cd14601"
      },
      "source": [
        "## 3.1 Split the data into training set and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a74cd9c",
      "metadata": {
        "id": "1a74cd9c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2\n",
        "from skimage.feature import local_binary_pattern #opencv2 does not expose LBP\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "##---------Code begins below this line------------------##\n",
        "\n",
        "# Shared dataset link not working with gdown library due to permissions\n",
        "# The images have been downloaded and uploaded to Google drive(My).\n",
        "\n",
        "# The below steps necessary as on reconnect, Google Colab deletes the content folder\n",
        "\n",
        "colab_folder = \"/content\"  # Default working directory in Google Colab\n",
        "zip_file_name = \"/content/drive/MyDrive/scene_classification.zip\"\n",
        "\n",
        "# Decompress the zip file, using with instead of try/finally\n",
        "# Check if data folder exists; if it exists, then the dataset has been extracted\n",
        "if not os.path.exists(os.path.join(colab_folder, \"data\")):\n",
        "\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')  # Approve access for Google desktop app\n",
        "\n",
        "    print(\"Decompressing the zip file...\")\n",
        "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "        extract_path = os.path.join(colab_folder, \"data\")\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Decompression complete. Files extracted to {extract_path}\")\n",
        "\n",
        "# Find the number of images for each category\n",
        "for category in os.listdir(os.path.join(colab_folder, \"data\", \"subset\")):\n",
        "    category_path = os.path.join(colab_folder, \"data\", \"subset\", category)\n",
        "    if not os.path.isdir(category_path):\n",
        "        continue\n",
        "    num_images = len(os.listdir(category_path))\n",
        "    print(f\"Category: {category}, Number of Images: {num_images}\")\n",
        "\n",
        "# Define constants\n",
        "NUM_IMAGES_PER_FOLDER = 500\n",
        "DATASET = []\n",
        "\n",
        "# Features to extract from images - Color, Shape, and Texture\n",
        "\n",
        "# Function to resize Image, convert all images to standard size\n",
        "def resize_image(image):\n",
        "    IMAGE_SIZE = (128, 128)  # Resize to 128x128\n",
        "    resized_image = cv2.resize(image, IMAGE_SIZE)\n",
        "    return resized_image\n",
        "\n",
        "# Function to extract color details\n",
        "def extract_color_details(image):\n",
        "    # OpenCV loads in BGR format, calculate histogram of intensity\n",
        "    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
        "    hist = cv2.normalize(hist, hist).flatten()\n",
        "    return hist\n",
        "\n",
        "# Function to extract texture using LBP\n",
        "def extract_texture_LBP(gray_image):\n",
        "    # Compute the LBP of the image, using skimage.feature.localBinaryPattern\n",
        "    lbp = local_binary_pattern(gray_image, 8, 3.0, method=\"uniform\")\n",
        "    return lbp.flatten()\n",
        "\n",
        "# Function to extract texture using Gabor filters\n",
        "def extract_texture_gabor(gray_image, psi):\n",
        "    gabor_kernel = cv2.getGaborKernel(ksize=(5, 5), sigma=5.0, theta=0.0, lambd=10.0, gamma=0.5, psi=psi)\n",
        "    filtered_image = cv2.filter2D(gray_image, cv2.CV_8UC3, gabor_kernel)\n",
        "    mean = np.mean(filtered_image)\n",
        "    std_dev = np.std(filtered_image)\n",
        "    return [mean, std_dev]\n",
        "\n",
        "# Function to extract shape features\n",
        "def extract_edge_features(gray_image):\n",
        "    edge_image = cv2.Canny(gray_image, threshold1=50, threshold2=150)\n",
        "    no_of_edge_pixel = np.sum(edge_image > 0)\n",
        "    return no_of_edge_pixel / gray_image.size\n",
        "\n",
        "# Path to the base folder containing category subfolders\n",
        "base_folder = os.path.join(colab_folder, \"data\", \"subset\")\n",
        "\n",
        "# Process dataset and extract features\n",
        "for category in os.listdir(base_folder):\n",
        "    category_path = os.path.join(base_folder, category)\n",
        "    if not os.path.isdir(category_path):\n",
        "        continue\n",
        "\n",
        "    image_count = 0\n",
        "    for image_file in os.listdir(category_path):\n",
        "        if image_count >= NUM_IMAGES_PER_FOLDER:\n",
        "            break\n",
        "\n",
        "        image_path = os.path.join(category_path, image_file)\n",
        "        if not os.path.isfile(image_path):\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        dominant_colors = extract_color_details(image)\n",
        "        texture_lbp = extract_texture_LBP(gray_image)\n",
        "        texture_gabor_0 = extract_texture_gabor(gray_image, 0)\n",
        "        edge_density = extract_edge_features(gray_image)\n",
        "\n",
        "        DATASET.append({\n",
        "            \"img_name\": image_path,\n",
        "            \"label\": category,\n",
        "            \"dominant_colors\": dominant_colors,\n",
        "            \"texture_lbp\": texture_lbp,\n",
        "            \"texture_gabor_0\": texture_gabor_0,\n",
        "            \"edge_density\": edge_density\n",
        "        })\n",
        "        image_count += 1\n",
        "\n",
        "# Convert dataset to a DataFrame\n",
        "df = pd.DataFrame(DATASET)\n",
        "\n",
        "# Save dataset to Parquet\n",
        "df.to_parquet('image_dataset.parquet', compression='snappy')\n",
        "print(\"Dataset created and saved to 'image_dataset'\")\n",
        "\n",
        "# Dataset Size\n",
        "print(f\"Size of the dataset: {len(df)} images\")\n",
        "\n",
        "# Plotting category distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y=\"label\", data=df, order=df[\"label\"].value_counts().index)\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_image(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    equalized_image = cv2.equalizeHist(gray_image)\n",
        "    return equalized_image\n",
        "\n",
        "# Splitting the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "print(f\"Training set size: {len(train_df)} images\")\n",
        "print(f\"Testing set size: {len(test_df)} images\")\n",
        "\n",
        "# Feature extraction\n",
        "def extract_features(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    gray_image = preprocess_image(image)\n",
        "\n",
        "    # Extract features\n",
        "    dominant_colors = extract_color_details(image)\n",
        "    texture_lbp = extract_texture_LBP(gray_image)\n",
        "    edge_density = [extract_edge_features(gray_image)]  # Ensure this is a list\n",
        "\n",
        "    # Check feature lengths\n",
        "    try:\n",
        "        # Concatenate features ensuring consistent lengths\n",
        "        return np.concatenate([dominant_colors, texture_lbp, edge_density])\n",
        "    except ValueError as e:\n",
        "        print(f\"Error concatenating features for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Calculate expected feature length\n",
        "sample_image_path = os.path.join(base_folder, os.listdir(base_folder)[0], os.listdir(os.path.join(base_folder, os.listdir(base_folder)[0]))[0])\n",
        "sample_image = cv2.imread(sample_image_path)\n",
        "gray_sample_image = preprocess_image(sample_image)\n",
        "expected_feature_length = len(extract_color_details(sample_image)) + len(extract_texture_LBP(gray_sample_image)) + 1\n",
        "\n",
        "# Process training and testing sets\n",
        "def process_data(df):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        feature = extract_features(row[\"img_name\"])\n",
        "        if feature is not None and feature.shape[0] == expected_feature_length:  # Check expected length\n",
        "            features.append(feature)\n",
        "            labels.append(row[\"label\"])\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "train_features, train_labels = process_data(train_df)\n",
        "test_features, test_labels = process_data(test_df)\n",
        "\n",
        "# Normalizing the features\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "if train_features.shape[1] > 500:\n",
        "    pca = PCA(n_components=500)\n",
        "    train_features = pca.fit_transform(train_features)\n",
        "    test_features = pca.transform(test_features)\n",
        "    print(f\"Reduced feature vector size: {train_features.shape[1]}\")\n",
        "\n",
        "# Save features\n",
        "np.save('train_features.npy', train_features)\n",
        "np.save('train_labels.npy', train_labels)\n",
        "np.save('test_features.npy', test_features)\n",
        "np.save('test_labels.npy', test_labels)\n",
        "\n",
        "print(\"Processed features saved for training and evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3cec4fc",
      "metadata": {
        "id": "e3cec4fc"
      },
      "source": [
        "## 3.2 Feature Engineering -- Score: 3.5 Marks\n",
        "\n",
        "* Extract the features from the images and concatenate them to create a single for the every images.\n",
        "\n",
        "* You can choose from the feature processing techniques taught in the class : Low-level Vision: Histogram and Histogram equalization, Gray-scale transformation, Image Smoothing, Connected components in images.\n",
        "Mid-level Vision:  Edge Detection using Gradients, Sobel, Canny; Line detection using Hough transforms; Semantic information using RANSAC;Image region descriptor using SIFT; Use case: Pedestrian detection Using HoG and SIFT descriptors and SVM\n",
        "\n",
        "* Create multiple sets of features and store it in seperate dataframes so that you can later use it for training and comparing the models.\n",
        "\n",
        "* Normalize the DataFrame\n",
        "\n",
        "* Note : If the feature size is getting too large such that it is not fitting into the RAM of Colab or your system then you can either use PCA or resize the image to smaller dimenssion for reducing the numer of features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c77697",
      "metadata": {
        "id": "71c77697"
      },
      "outputs": [],
      "source": [
        "##---------Type the answer below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae0b5d2",
      "metadata": {
        "id": "3ae0b5d2"
      },
      "source": [
        "# 4. Model Building - Score: 2.0 Marks\n",
        "\n",
        "## 4.1 Model Building - Score: 1.5 Marks\n",
        "* Use any 1 classical machine learning algorithm such as : SVM , Xgboost etc. to train the model\n",
        "* Train the model on different kinds of feature combination dataframe you created in 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868d7b27",
      "metadata": {
        "id": "868d7b27"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575f9e37",
      "metadata": {
        "id": "575f9e37"
      },
      "source": [
        "## 4.2 Validation matrix - Score: 0.5 Marks\n",
        "\n",
        "Print the model accuracy and F1 Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad56d90",
      "metadata": {
        "id": "6ad56d90"
      },
      "outputs": [],
      "source": [
        "##---------Type the answer below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdbc82a1",
      "metadata": {
        "id": "bdbc82a1"
      },
      "source": [
        "# 5. Model Inference & Evaluation - Score: 1 Mark\n",
        "\n",
        "Plot any 5 random test images and their predicted and actual true labels using the model and feature set which gave you the best accuracy/F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85e9754",
      "metadata": {
        "id": "a85e9754"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19bd0c56",
      "metadata": {
        "id": "19bd0c56"
      },
      "source": [
        "Justify your choice/inution of feature selection based on the performance of model such that why a particualr set have features might have performed well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89d2981",
      "metadata": {
        "id": "b89d2981"
      },
      "outputs": [],
      "source": [
        "##---------Type the answers below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Documentation, Study presentation and Code Quality -- Score: 1.5 Marks"
      ],
      "metadata": {
        "id": "rJBZ1ovpKgAR"
      },
      "id": "rJBZ1ovpKgAR"
    },
    {
      "cell_type": "markdown",
      "id": "RcDDQlfbZQ7E",
      "metadata": {
        "id": "RcDDQlfbZQ7E"
      },
      "source": [
        "### NOTE\n",
        "\n",
        "\n",
        "All Late Submissions will incur a <b>penalty of -2 marks </b>. So submit your assignments on time.\n",
        "\n",
        "Good Luck"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}